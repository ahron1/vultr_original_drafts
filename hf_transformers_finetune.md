# Fine-tune a Hugging Face Transformer Model on Vultr Cloud GPU

## Introduction

Large Language Models (LLMs), such as OpenAI's GPT, TII's Falcon, EleutherAI's GPT-Neo and GPT-J, and others, have attracted public attention for their broad-based capabilities. LLMs are a type of transformer model. Transformer models are typically trained on large volumes of data over many hours-days, using many GPUs and TPUs. The models mentioned earlier are pre-trained - they are ready to use as-is. Pre-trained models are general-purpose models, they do not excel at any specific task. Their output is based on the dataset(s) they were trained on.

A model's output depends on its weights. An untrained model has random weights. The (untrained) model's outputs are also random. The training process updates the weights until the model's output matches the training goals. You can also update a model's weights so that it performs better at a specific task. To do this, train the model on a dataset specific to the task it needs to perform well on. 

For example, to generate text in a specific language, the model needs to be trained on a dataset of text in that language. Similarly, to make a model that mimics a subject matter expert on a specific topic, it needs to be trained on resources on that topic. 

The two main approaches to train a model on a specific dataset are:
* Train a model from scratch on the desired dataset. This is a resource-intensive process.
* Start with a pre-trained model and train it further on the desired dataset. This is more efficient than starting with random weights. It is called fine-tuning (a pre-trained model) and is the topic of this article.

### Prerequisites and Scope 

This article shows how to fine-tune transformer models using HuggingFace. It explains the process and demonstrates it with examples. To follow this article, you are expected to be comfortable with [using HuggingFace Transformer models](https://www.vultr.com/docs/how-to-use-hugging-face-transformer-models-on-vultr-cloud-gpu/). You are also expected to be familiar with intermediate Python programming. It is assumed you have access to a computer with a powerful GPU chip or a [cloud server with GPU capabilities](https://www.vultr.com/products/cloud-gpu/). It is advisable to use [Tmux](https://www.vultr.com/docs/how-to-install-and-use-tmux/) or [Screen](https://www.vultr.com/docs/an-introductory-guide-to-screen/). These tools make it possible to run the code in one pane and simultaneously monitor the system in another pane. They also preserve long running sessions in case the SSH connection gets dropped.

The examples in this article are tested on fresh Debian 11 servers with NVIDIA A100 and A40 GPUs. They should work on all recent Linuxes with NVIDIA GPUs.

This article illustrates fine-tuning with hands-on examples using EleutherAI's GPT-Neo. It explains the steps to fine-tune GPT-Neo (with 125 million parameters) using the Netflix dataset. It also describes how to adapt the process to fine-tune the GPT-Neo models with 1.3 billion and 2.7 billion parameters. The training dataset consists of information about Netflix shows. The models are fine-tuned on the show descriptions. The text generated by the models after fine-tuning is expected to have the "tone" of a show/movie description.

Training even larger models, such as Falcon and GPT-J, on standard A100 GPUs, involves the use of specific techniques like Low Rank Adaptation (LoRA). These techniques are beyond the scope of this article.

### Fine-tuning Overview 

To fine-tune a model, you need the following:

* A system with the necessary hardware and software
* A pre-trained model
* A dataset on which to train (fine-tune) and evaluate the pre-trained model 
* A tokenizer to convert (tokenize) the dataset into a format (for example, arrays and tensors of numbers) that the model can use. Models are unable to use raw data (for example, text) directly.
* A metric to evaluate the model's performance during the training process
* A training function to train the model 

The following sections describe and illustrate the above points.

## Hardware Considerations

Training (fine-tuning) models is a GPU-heavy task. Larger models have a higher number of parameters (weights) and correspondingly high GPU requirements.

The relatively small GPT-Neo model with 125 million parameters needs up to 1 GB to load in memory. To fine-tune this model on the Netflix dataset, the system needs over 8 GB of GPU RAM using a batch size of 1. The examples in this article are based on a batch size of 1. To try the examples, it is recommended to use a machine with 10 GB GPU RAM.

For production use, it is recommended to fine-tune the model using a somewhat higher batch size, like 4 or 8. This increases the memory requirements. With a batch size of 4, the GPU needed goes up to 26 GB. 8 batches take over 50 GB GPU RAM. A machine with a single A100 GPU is enough for this. 

The larger GPT-Neo-1.3B model with 1.3 billion parameters takes around 35 GB GPU to fine-tune with a batch size of 1. It takes around 55 GB with a batch size of 2. With a batch size of 2 and accumulating gradients every 8 steps, it takes around 60 GB. 

GPT-Neo-2.7B with 2.7 billion parameters needs over 65 GB GPU with a batch size of 1. On a machine with 1 A100 80GB GPU, it trains (on the same Netflix dataset) at a rate of around 0.4 samples per second.

If the server has insufficient memory to handle the model, the process terminates with an out of memory (OOM) error. OOM errors commonly look like this:

    torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 80.00 GiB total capacity; 73.26 GiB already allocated; 98.19 MiB free; 73.57 GiB reserved in total by PyTorch)

The command `nvidia-smi` shows the system's GPU usage. Run this command at the command prompt using `watch` to monitor the system's (near real-time) GPU usage.

    $ watch nvidia-smi

## Set up the System (Software)

Follow the steps below to prepare a fresh Debian system with the software needed to fine-tune HuggingFace transformer models using the GPU. Run these commands at the command prompt (terminal). 

Download the shell script to install Conda:

    $ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

Run the script: 

    $ bash Miniconda3-latest-Linux-x86_64.sh

Follow the on-screen instructions and finish installing Conda.

When the installation completes, log out of the server and log back in. This activates Conda.

Upgrade Conda: 

    $ conda upgrade -y conda

Create a new Conda environment `env1`:

    $ conda create -y --name env1 python=3.9 

`env1` is installed with Python 3.9. Activate `env1`:

    $ conda activate env1

Upgrade `pip`:

    $ pip install --upgrade pip

Install the CUDA GPU packages with Conda:

    $ conda install -y -c conda-forge cudatoolkit=11.8 cudnn=8.2

Install Pytorch and its NVIDIA dependencies: 

    $ conda install -y -c pytorch -c nvidia pytorch=2.0.1 pytorch-cuda=11.8 

Export the paths to the Conda libraries:

    $ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/

Create the Conda activation scripts directory:

    $ mkdir -p $CONDA_PREFIX/etc/conda/activate.d

Append the paths of Nvidia tools to the Conda activation script: 

    $ echo 'CUDNN_PATH=$(dirname $(python -c "import nvidia.cudnn;print(nvidia.cudnn.__file__)"))' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh

    $ echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh

Activate Conda:

    $ source $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh

Use Pip to install [`tensorflow`](https://pypi.org/project/tensorflow/), [`transformers`](https://pypi.org/project/transformers/), [`huggingface-hub`](https://pypi.org/project/huggingface-hub/), Nvidia tools and, a few other dependencies like [`einops`](https://pypi.org/project/einops/), [`accelerate`](https://pypi.org/project/accelerate/) and, [`xformers`](https://pypi.org/project/xformers/):

    $ pip install nvidia-cudnn-cu11==8.6.0.163 tensorflow==2.12.* transformers==4.30.* huggingface-hub einops==0.6.1 accelerate==0.20.3 xformers==0.0.20 scikit-learn==1.3.0 evaluate==0.4.0

Test a GPU-based command in Python: 

    $ python3 -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"

If Python has been installed with GPU support, the above command's output resembles:

    [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

The commands in this section are based on the official documentation of [Tensorflow](https://www.tensorflow.org/install/pip), [PyTorch](https://pytorch.org/get-started/locally/), and [Nvidia](https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html). 

Start a Python shell before proceeding further with the examples.

    $ python

## Fine-tune GPT-Neo-125M

GPT-Neo-125M is smaller than other similar models and hence has lower memory requirements. Thus, it is a more beginner-friendly learning tool. To try the code in this section, use a machine with 10 GB or more GPU RAM. Changing the configuration parameters affects memory requirements.

Import the necessary packages into the Python shell:

    >>> from transformers import Trainer, TrainingArguments, AutoTokenizer, DataCollatorForLanguageModeling
    >>> from transformers import GPTNeoForCausalLM
    >>> from datasets import load_dataset
    >>> import torch, evaluate, sklearn, numpy as np

Load the model and move it to GPU:

    >>> model = GPTNeoForCausalLM.from_pretrained(
            "EleutherAI/gpt-neo-125m", 
            low_cpu_mem_usage=True,
        ).cuda()

Set the padding token to be equal to the end-of-sentence (EOS) token:

    >>> model.config.pad_token_id = model.config.eos_token_id

The above step is optional. If you don't manually set this, the system does it automatically, but gives a warning when running the model:

    # Error-message:
    # Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.

Import the tokenizer corresponding to the model:

    >>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-125m")

As with the model, manually set the EOS token equal to the padding token for the tokenizer:

    >>> tokenizer.pad_token = tokenizer.eos_token

Load the dataset:

    >>> dataset = load_dataset("hugginglearners/netflix-shows")

The above command loads the [`netflix-shows` dataset from the `HuggingLearners` repository on HuggingFace](https://huggingface.co/datasets/hugginglearners/netflix-shows). Fetch the keys of an arbitrary item and check the data's structure:

    >>> dataset["train"][100].keys() 

Check the data itself:

    >>> dataset["train"][100]

The fine-tuning process uses the text in the `description` field. This field consists of the descriptions of Netflix shows.

Define a tokenizing function:

    >>> def tokenize_function(examples):
            tokenized_data = tokenizer(examples["description"], padding="max_length", truncation=True)
            return tokenized_data

This tokenizing function calls the tokenizer (imported earlier) and applies it to the data item's description field. Simultaneously, it also [pads and truncates](https://huggingface.co/docs/transformers/pad_truncation) the input (description field). This is necessary because models need standardized inputs. Each input must be of the same length. The maximum size (length) of each input is also limited. However, in real-world data, the inputs are of different lengths. They can also be longer than the maximum length the model accepts. Padding and truncating fix this.

To tokenize the dataset, use Python's `map` method to apply the tokenizing function on each item in the dataset:

    >>> tokenized_dataset = dataset.map(tokenize_function, batched=True)

Create two subsets from the tokenized dataset: a training dataset and an evaluation dataset:

    >>> train_dataset = tokenized_dataset["train"].select(range(0,7999))
    >>> eval_dataset = tokenized_dataset["train"].select(range(8000,8799))

The original dataset has over 8800 items. The training dataset consists of 8000 items and the evaluation dataset has 800. 

In practice, it is advisable to first test the entire process with a smaller dataset. Create sample training and evaluation datasets (with, for example, around 100 items for training, and 20 for evaluating). 

    >>> # train_dataset = tokenized_dataset["train"].select(range(0,99)) 
    >>> # eval_dataset = tokenized_dataset["train"].select(range(100,119))

A small dataset, as shown above, is not useful for training (fine-tuning). However, it helps to test the entire process before applying it to the complete dataset. The example code directly uses the full dataset, not the smaller samples.

Specify the output directory in which to store model checkpoints:

    >>> output_dir =  "./fine_tuned_models/gpt-neo-125m"

Define a set of training arguments - these are the parameters passed to the training function.

    >>> training_args = TrainingArguments(
            output_dir = output_dir,
            logging_dir='./logs',
            label_names=['input_ids', 'attention_mask'], 
            per_device_train_batch_size=1,
            gradient_accumulation_steps=1,
            num_train_epochs=1,
        )

Below is a short description of the parameters used in the training arguments:

* The output directory stores the model checkpoints.
* The logging directory stores the training logs. 
* `label_names` are the labels used to label the data.
* `per_device_train_batch_size` specifies the batch size. In this case, the example uses a batch size of 1. If this parameter is not specified, the training process uses the default batch size of 8. Larger batch sizes need more GPU. The batch size is *per GPU*. If the machine has 4 GPUs, and this parameter is set to 1, each GPU (in parallel) processes 1 batch. Larger batch sizes generally lead to better training. However, a batch size too large leads to instability in the training process.
* `gradient_accumulation_steps` is the number of steps for which the gradient should be accumulated. This allows you to apply [gradient accumulation](https://www.vultr.com/docs/how-to-use-gradient-accumulation-to-overcome-gpu-memory-limitations/) during the training process. Standard GPUs cannot handle larger batch sizes. Sometimes, the GPU can only handle a batch size of 1. Gradient accumulation is a way to partially replicate the (positive) effects of larger batch sizes while still training with a smaller batches. The default value is 1. Experiment with using a value of 4, 8, or 16.
* `num_train_epochs` specifies the number of epochs. The higher the number of epochs, the longer the processing time. If this parameter is not specified, it uses the default value of 3 epochs. Having too few epochs can result in an untrained model. Too many epochs can result in overfitting. The examples in this article run the training for a single epoch. This is inappropriate in a production setting.

It is also possible to specify the `learning_rate`. If the learning rate is too high, it results in the model converging to a local optimum, and thus being insufficiently trained. A low learning rate takes longer to converge to the optimum. When the `learning_rate` is not specified, as in this example, the default value of `5e-05` is used.

Performance of the training (fine-tuning) process depends on many parameters. Configuring these parameters with the right values is partly subjective and depends on experience. It also varies from model to model. This is called optimizing the training process.

Define a metric:

    >>> metric = evaluate.load("accuracy")

This metric is loaded using HuggingFace's [Evaluate module](https://huggingface.co/docs/evaluate/a_quick_tour). During the training process, the model's performance is measured based on this metric. The metric can be based on [different parameters](https://scikit-learn.org/stable/modules/model_evaluation.html). In this case, the parameter is "accuracy".

Define a function `compute_metrics` which uses the metric defined above:

    >>> def compute_metrics(prediction_to_evaluate):
            logits, labels = prediction_to_evaluate
            prediction = np.argmax(logits, axis=-1)
            return metric.compute(predictions=prediction, references=labels)


This function is passed to the trainer as a parameter. The trainer uses it to compute the loss and measure (evaluate) the performance (accuracy) of each output (prediction).

Datacollators are special functions that tell the trainer how to form batches from a list of input elements. In this case, it is necessary to specify a collator specific to language models:

    >>> data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

The collator accepts the tokenizer as an input parameter. The parameter `mlm` specifies whether masked language modeling is used. Masked language modeling is useful in applications like translation. In this case, to build a generative model, set the `mlm` parameter to false.

Using the training arguments, the `compute_metrics` function, and the data collator, define the trainer function:

    >>> trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            compute_metrics=compute_metrics,
            data_collator=data_collator,
        )

Run the training, and assign the output to a variable, `result`:

    >>> result = trainer.train()

The training (fine-tuning) can take a while - on a machine with 2 vCPUs and 10 GB GPU, it takes around 3 hours for the Netflix dataset. After the training finishes, check the details of the training process:

    >>> result.metrics

This shows the time taken for the training, the rate of training (samples per second), the loss, and other information. Monitor these results while trying to improve the fine-tuning by varying the `training_args` parameters (for example, `gradient_accumulation_steps`, `per_device_train_batch_size`, and so on.).

Save the trained model with an appropriate name:

    >>> trainer.save_model('vultr/finetuned_gpt_neo_125_netflix')

Exit and restart the Python shell.

### Use the Fine-tuned model

In the new Python session, import the required modules:

    >>> from transformers import pipeline, AutoTokenizer
    >>> from transformers import GPTNeoForCausalLM 

Import the tokenizer for the model:

    >>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-125m")

Both the pre-trained and fine-tuned models use the same tokenizer. Import the default model:

    >>> model = GPTNeoForCausalLM.from_pretrained(
            "EleutherAI/gpt-neo-125m", 
            low_cpu_mem_usage=True,
        ).cuda()

Import the fine-tuned model (saved earlier):

    >>> model_finetuned = GPTNeoForCausalLM.from_pretrained(
            "vultr/finetuned_gpt_neo_125_netflix", 
            low_cpu_mem_usage=True,
        ).cuda()

Define a pipeline with the default model:

    >>> generator = pipeline(task="text-generation", model=model, tokenizer=tokenizer, device=0)

Define a pipeline with the fine-tuned model:

    >>> generator_finetuned = pipeline(task="text-generation", model=model_finetuned, tokenizer=tokenizer, device=0)

Declare a few input prompts:

    >>> prompt = "One fine morning" 
    >>> # prompt = "A boy and a girl"
    >>> # prompt = "Vultr is a cloud service provider"

Generate text using the default model:

    >>> generator(prompt, max_length=200, do_sample=True, num_return_sequences=1)

Generate text using the fine-tuned model:

    >>> generator_finetuned(prompt, max_length=200, do_sample=True, num_return_sequences=1)

Try a few other prompts and run each prompt a few times.

Compare the outputs of the pre-trained and the fine-tuned model. Observe, qualitatively, the difference in the outputs. Notice that the fine-tuned model's output reads more like a story.

## Fine-tune Larger GPT-Neo Models

The process of fine-tuning and using larger GPT-Neo models is the same. In the above code samples, replace `EleutherAI/gpt-neo-125m` with `EleutherAI/gpt-neo-1.3b` to use the model with 1.3 billion parameters. Use `EleutherAI/gpt-neo-2.7b` for the model with 2.7 billion parameters. Save the fine-tuned models with an appropriate name. Both these models can be fine-tuned on a single A100 80 GB GPU.

## Conclusion

Fine-tuning transformer models is a large and complex topic. This article presents an introduction to the principles of fine-tuning, in particular LLMs. It demonstrates fine-tuning with a new example. This example fine-tunes a pre-trained LLMs (GPT-Neo with 125 million parameters) on a HuggingFace dataset consisting of Netflix show descriptions. Given an input prompt, the resultant models are capable of generating text in the style of a Netflix show description.

It is critical to understand that an LLM model, fine-tuned or otherwise, merely generates coherent text that sounds similar to the training corpus. Current LLM models are not concerned with the meaningfulness or correctness of their outputs. The training process optimizes for generating text, not for acquiring or understanding knowledge. While a fine-tuned LLM can be a valuable tool for summarizing text, sentence completion, answering questions, and the like, the outputs should be subject to human scrutiny.

